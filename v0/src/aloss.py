'''
Implementation of Optimal Subset Selection for Active Learning
'''
import numpy as np
import time
import random
from scipy.spatial.distance import cdist


def greedy_solver(M, k):
    sums = np.sum(M, axis=1)
    zipped_sums = list(enumerate(sums))
    sorted_sums = sorted(zipped_sums, key=lambda x: x[1], reverse=True)
    indices = [i for (i, d) in sorted_sums[:k]]
    return indices

#just to make sure our thing is good, we run brute force sol
def brute_force_solver(M, k):
    # given input x, generate all arrays with x 1's of length n
    def get_vectors(x, n):
        if x == 0: return [[0 for _ in range(n)]]
        if n == 0: return None
        res = []
        #res1 will return all arrays with x 1's of length n - 1.
        #we append 0's to these puppies
        res1 = get_vectors(x, n-1)
        #res2 will return all arrays with x-1 1's of length n - 1.
        #we append 1's to these kitties
        res2 = get_vectors(x-1, n-1)
        if res1 == None and res2 == None:
            return None
        if res1 != None:
            for arr in res1:
                res.append(arr + [0])
        if res2 != None:
            for arr in res2:
                res.append(arr + [1])
        return res
        # for arr in get_vectors(x-1, n-1):
            # res.append(arr)
    vectors = get_vectors(k, len(M))
    maxIndex = None
    maxVal = 0
    for i, e in enumerate(vectors):
        ve = np.array(e)
        vi = np.matmul(np.matmul(ve.transpose(), M), ve)
        if vi > maxVal:
            maxIndex = i
            maxVal = vi
    return vectors[maxIndex]

# M = [
#     [9,0,0,0,0],
#     [0,0,0,0,0],
#     [0,0,9,0,0],
#     [0,0,0,8,0],
#     [0,0,0,0,0],
# ]

# size = 20
# M = np.random.rand(size, size)
#
def compute_wei(e, M):
    ve = np.array(e)
    return np.matmul(np.matmul(ve.transpose(), M), ve)
#
# k = 5

# now = time.time()
# wei2 = find_max_e(M, k)
# wei2 = approx_solver(M, k)
# print("my sol took", time.time() - now)
# now = time.time()
# wei1 = brute_force_solver(M, k)
# print("brute force took", time.time() - now)
# print("actual sol\t", compute_wei(wei1, M), wei1)
# print("my sol\t\t", compute_wei(wei2, M), wei2)
# a = np.array([1,0,1,0,0])
# b = np.matmul(a.transpose(), M)
# d = np.matmul(b, a)
# print(d)

def instance_disparities(d):
    return cdist(d, d, 'euclidean')
    # n = len(d)
    # print("wei-1", n, d)
    # return
    # #m1 has repeated columns
    # m1 = np.tile(d, (n,1))
    # print("wei0")
    # #m2 has repeated rows
    # m2 = np.tile(np.array([d]).transpose(), (1,n))
    # print("wei1")
    # diffs = m1 - m2
    # print("wei2")
    # disparity = diffs * diffs
    # print("wei3")
    # return disparity / np.max(disparity)



# returns euclidean distance between the two feature vectors
def instance_disparity(f1, f2):
    diffs = f1 - f2
    diffs *= diffs
    return sum(diffs)

# returns all uncertainties generated by our classifier
def instance_uncertainties(clf, features):
    probs = np.min(clf.predict_proba(features), axis=1)
    # probs = min(clf.predict_proba([f])[0])
    return probs

vec1 = [1,2,3]
vec2 = [1,2,13]

print(instance_disparity(np.array(vec1), np.array(vec2)))
